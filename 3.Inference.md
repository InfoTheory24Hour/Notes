
### Inference as inverse probability


#### 1.  When we use probability, it does not mean the case we are studying has some stochastic aspects. It only indicates the hardness to find the unique, true one from those "mutually exclusive micro-hypotheses".

Fig 3.2
likelihood function  

Exercise: 3.15
Tossing the Belgian coin 250 times, Head 140 times, 110 tails. Does it indicate that the coin is biased?

**There are micro-hypotheses here, e.g. the coin is unbiased, biased with P = 0.500001 H/ 0.499999 T, biased with P = 0.50001 H/0.49999T down**

Consider two micro-hypotheses, $H_1$ and $H_0$

D: Observed Data
$H_1$: Assumption, biased
$H_0$: Assumption: unbiased.

\[
\frac{P(D|H_1)}{P(D|H_0)} = 0.48
\]

The range is (0.25 - 1.9), namely the data here does not strongly support the biased hypotheses.

#### 2. Making some assumptions is the first step in any inference process. We can not avoid that.

Eg. 1
- Test Accuracy: 99%
- Really Rare disease 1/100000

If you get the positive result, How much possible you are sick?  

~1%

Why: 100000 * 100 population
100 patient, 99 detected. while 999900 0.01 = 9999 be false positives.

> Always write down the probability of everything.


#### 3. Bayes' theorem provides us an approach to incrementally "fix" our guesstamation of the model

Exercise 3.8

- Door 1, 2, 3
- Chose Door 1
- gameshow host opens door 3

Exercise 3.9

- Door 1, 2, 3
- Chose Door 2
- earthquake, Door 3 opens

Question 1 : What about when confused host forgets the result?

Question 2 : What have we learnt from this problem?
